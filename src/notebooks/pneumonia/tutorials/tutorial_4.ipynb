{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning "
      ],
      "metadata": {
        "id": "D9adKc58rL99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is the technique to transfer what has been learned previously to new related tasks. Generally, traditional models work in isolation. Transfer learning overcomes the isolated learning paradigm and utilises knowledge acquired for one problem to solve related ones. \n",
        "\n",
        "Transfer learning is usually expressed through the use of pre-trained models. A pre-trained model is a model that was trained on a large benchmark dataset(like ImageNet) to solve a problem similar to the one that we want to solve.\n",
        "\n",
        "When you are repurposing a pre-trained model for your own needs, you start by removing the original classifier. You add a new classifier that fits your purposes, and finally, you have to fine-tune your model according to one of three strategies:\n",
        "* Train the entire model. In this case, you use the architecture of the \n",
        "pre-trained model and train it according to your dataset. You are training the model from scratch and thus, need a large dataset (and much computational power).\n",
        "* Train some layers and leave the others frozen. As you remember, lower layers refer to general features (problem independent), while higher layers refer to specific features (problem-dependent). Here, we play with that dichotomy by choosing how much we want to adjust the network's weights (a frozen layer does not change during training). Usually, if you have a small dataset and many parameters, you will leave more layers frozen to avoid overfitting. By contrast, if the dataset is large and the number of parameters is small, you can improve your model by training more layers to the new task since overfitting is not an issue.\n",
        "* Freeze the convolutional base. This case corresponds to an extreme situation of the train/freeze trade-off. The main idea is to keep the convolutional base in its original form and then use its outputs to feed the classifier. You are using the pre-trained model as a fixed feature extraction mechanism, which can be helpful if you are short on computational power, your dataset is small, and/or a pre-trained model solves a problem very similar to the one you want to solve.\n"
      ],
      "metadata": {
        "id": "fo5ru2ajrWtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A sample code explaination using VGG16 pre-trained model"
      ],
      "metadata": {
        "id": "aL9RZfe4vNhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Initialize the Pretrained Model\n",
        "feature_extractor = VGG16(weights='imagenet', \n",
        "                             input_shape=(224, 224, 3),\n",
        "                             include_top=False)\n",
        "\n",
        "# Set this parameter to make sure it's not being trained\n",
        "feature_extractor.trainable = False\n",
        "\n",
        "# Set the input layer\n",
        "input_ = tensorflow.keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "# Set the feature extractor layer\n",
        "x = feature_extractor(input_, training=False)\n",
        "\n",
        "# Set the pooling layer\n",
        "x = tensorflow.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Set the final layer with sigmoid activation function\n",
        "output_ = tensorflow.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the new model object\n",
        "model = tensorflow.keras.Model(input_, output_)\n",
        "\n",
        "# Compile it\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# Print The Summary of The Model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRU5BRS4v3Xo",
        "outputId": "f53e21cf-2a97-4da5-d49f-0740ad9d9fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 512)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,715,201\n",
            "Trainable params: 513\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used VGG16 as the backbone for our new model. We have created the input layer and changed the final linear layer of ResNet-50 with the new one based on the number of classes. Note that we set the base model (VGG16) as non trainable because we are using the pre-trained weights of the base model."
      ],
      "metadata": {
        "id": "hfMq2ptzwYbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inception net\n",
        "Inception Net has several versions. \n",
        "They are as follows:\n",
        " * [Inception v1](https://arxiv.org/pdf/1409.4842v1.pdf)\n",
        " * [Inception v2 and Inception v3](https://arxiv.org/pdf/1512.00567v3.pdf)\n",
        " * [Inception v4 and Inception-ResNet](https://arxiv.org/pdf/1602.07261.pdf)\n",
        "\n",
        "Go through the papers if you want to know more about the architecture of Inception Net.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wl2NGn_MxwLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This week's work\n",
        "This week you have to train your transfer learning model using [InceptionNet v3](https://keras.io/api/applications/inceptionv3/) which is readily available in keras instead of VGG16 which has been shown here. "
      ],
      "metadata": {
        "id": "WBtKq98Hz4RK"
      }
    }
  ]
}